# Matrix multiplication on GPU - EXLA

```elixir
Mix.install(
  [
    {:nx, "~> 0.4.0"},
    {:scidata, "~> 0.1.9"},
    {:axon, "~> 0.3.0"},
    {:exla, "~> 0.4"}
  ],
  system_env: %{"XLA_TARGET" => "cuda111"}
)
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Before running notebook

This notebook has a dependency on EXLA.  You could have direct access to an NVidia GPU, AMD ROCm or a TPU.  According to the documentation, https://github.com/elixir-nx/nx/tree/main/exla#readme EXLA will try to find a precompiled version that matches your system.  If it doesn't find a match. you will need to install CUDA and CuDNN for your system.

The notebook is currently configured for Nvidia GPU via

```
system_env: %{"XLA_TARGET" => "cuda111"}
```

Review the configuration documentation for more options. https://hexdocs.pm/exla/EXLA.html#module-configuration

We had to install CUDA and CuDNN but that was several months ago.  Your experience may vary from ours.

## Experimenting with backend control

In this notebook, we are going to experiment with swapping out backends in the same notebook.  We chose not to set the backend globally throughout the notebook.  At the beginning of the notebook we'll repeat the approach we used in 01a_matmul_using_CPU.  We begin with the Elixir Binary backend.  You'll see that it isn't quick multiplying 10,000 rows of MNIST data by some arbitrary weights.  We'll then repeat the same multiplication using an NVidia 1080Ti GPU.  The 1080 Ti is not the fastest GPU, but it is tremendously faster than a "large" set of data on the BinaryBackend.

* 31649.26 milliseconds using BinaryBackend with a CPU only.
* 0.14 milliseconds using XLA with a warmed up GPU

*226,000 times faster on an old GPU*

```elixir
# Without choosing a backend, Nx defaults to Nx.BinaryBackend
Nx.default_backend()
```

<!-- livebook:{"output":true} -->

```
{Nx.BinaryBackend, []}
```

```elixir
# Just in case you rerun the notebook, let's make sure the default backend is BinaryBackend
# Setting to the Nx default backend
Nx.default_backend(Nx.BinaryBackend)
Nx.default_backend()
```

<!-- livebook:{"output":true} -->

```
{Nx.BinaryBackend, []}
```

We'll pull down the MNIST data again

```elixir
{train_images, train_labels} = Scidata.MNIST.download()
```

<!-- livebook:{"output":true} -->

```
{{<<0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...>>, {:u, 8}, {60000, 1, 28, 28}},
 {<<5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8,
    6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, ...>>, {:u, 8}, {60000}}}
```

```elixir
{train_images_binary, train_tensor_type, train_shape} = train_images
```

<!-- livebook:{"output":true} -->

```
{<<0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...>>, {:u, 8}, {60000, 1, 28, 28}}
```

```elixir
train_tensor_type
```

<!-- livebook:{"output":true} -->

```
{:u, 8}
```

Convert into Tensors and normalize to between 0 and 1

```elixir
train_tensors =
  train_images_binary
  |> Nx.from_binary(train_tensor_type)
  |> Nx.reshape({60000, 28 * 28})
  |> Nx.divide(255)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[60000][784]
  [
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...],
    ...
  ]
>
```

We'll separate the data into 50,000 train images and 10,000 validation images.

```elixir
x_train_cpu = train_tensors[0..49_999]
x_valid_cpu = train_tensors[50_000..59_999]
{x_train_cpu.shape, x_valid_cpu.shape}
```

<!-- livebook:{"output":true} -->

```
{{50000, 784}, {10000, 784}}
```

Training is more stable when random numbers are initialized with a mean of 0.0 and a variance of 1.0

```elixir
mean = 0.0
variance = 1.0
weights_cpu = Nx.random_normal({784, 10}, mean, variance, type: {:f, 32})
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[784][10]
  [
    [-0.5412212014198303, -1.5031448602676392, -0.9375351071357727, 0.8842236995697021, 1.2010047435760498, -0.2788948118686676, 0.09495414793491364, -1.530068039894104, -1.0373481512069702, 1.0582575798034668],
    [1.0582325458526611, 0.623931884765625, -2.1486785411834717, 0.33179396390914917, 0.39910393953323364, 0.36134496331214905, -1.6878384351730347, -1.3250863552093506, 0.9663388729095459, -1.22098708152771],
    [-1.0654748678207397, 0.3679066598415375, -1.8295735120773315, -0.333093523979187, 1.6659176349639893, 0.5998020768165588, -0.2500625252723694, -1.2196239233016968, 0.1907813549041748, -0.14031249284744263],
    [-0.8476660847663879, 0.3183368742465973, -0.01679874397814274, -0.33443644642829895, 0.2650495767593384, 0.4326851963996887, -0.673367440700531, -0.07346426695585251, -0.2923451066017151, 1.0891989469528198],
    [0.7539434432983398, -1.7904051542282104, -0.24815252423286438, -0.42723143100738525, -0.8594269752502441, -0.27156633138656616, 0.01748797297477722, 1.3395498991012573, 1.05559504032135, 0.5666554570198059],
    ...
  ]
>
```

```elixir
large_nx_mult_fn = fn -> Nx.dot(x_valid_cpu, weights_cpu) end
```

<!-- livebook:{"output":true} -->

```
#Function<43.3316493/0 in :erl_eval.expr/6>
```

```elixir
repeat = fn timed_fn, times -> Enum.each(1..times, fn _x -> timed_fn.() end) end
```

<!-- livebook:{"output":true} -->

```
#Function<41.3316493/2 in :erl_eval.expr/6>
```

```elixir
repeat_times = 5
# Warm up one iteration
{elapsed_time_micro, _} = :timer.tc(repeat, [large_nx_mult_fn, repeat_times])
avg_elapsed_time_ms = elapsed_time_micro / 1000 / repeat_times

{backend, _device} = Nx.default_backend()

"#{backend} CPU avg time in #{avg_elapsed_time_ms} milliseconds, total_time #{elapsed_time_micro / 1000} milliseconds"
```

<!-- livebook:{"output":true} -->

```
"Elixir.Nx.BinaryBackend CPU avg time in 31512.4698 milliseconds, total_time 157562.349 milliseconds"
```

Switch to the XLA backend and use the cuda device.  If you have a different device, replace all the :cuda specification with your device.

```elixir
Nx.default_backend({EXLA.Backend, device: :cuda})
Nx.default_backend()
```

<!-- livebook:{"output":true} -->

```
{EXLA.Backend, [device: :cuda]}
```

In the following section, we transfer the target data onto the GPU.

```elixir
x_valid_cuda = Nx.backend_transfer(x_valid_cpu, {EXLA.Backend, client: :cuda})
weights_cuda = Nx.backend_transfer(weights_cpu, {EXLA.Backend, client: :cuda})
```

<!-- livebook:{"output":true} -->

```

18:55:14.765 [info] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

18:55:14.767 [info] XLA service 0x7fabac028070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:

18:55:14.767 [info]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1

18:55:14.767 [info] Using BFC allocator.

18:55:14.767 [info] XLA backend allocating 10414463385 bytes on device 0 for BFCAllocator.

```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[784][10]
  EXLA.Backend<cuda:0, 0.3135309292.1542848544.244133>
  [
    [-0.5412212014198303, -1.5031448602676392, -0.9375351071357727, 0.8842236995697021, 1.2010047435760498, -0.2788948118686676, 0.09495414793491364, -1.530068039894104, -1.0373481512069702, 1.0582575798034668],
    [1.0582325458526611, 0.623931884765625, -2.1486785411834717, 0.33179396390914917, 0.39910393953323364, 0.36134496331214905, -1.6878384351730347, -1.3250863552093506, 0.9663388729095459, -1.22098708152771],
    [-1.0654748678207397, 0.3679066598415375, -1.8295735120773315, -0.333093523979187, 1.6659176349639893, 0.5998020768165588, -0.2500625252723694, -1.2196239233016968, 0.1907813549041748, -0.14031249284744263],
    [-0.8476660847663879, 0.3183368742465973, -0.01679874397814274, -0.33443644642829895, 0.2650495767593384, 0.4326851963996887, -0.673367440700531, -0.07346426695585251, -0.2923451066017151, 1.0891989469528198],
    [0.7539434432983398, -1.7904051542282104, -0.24815252423286438, -0.42723143100738525, -0.8594269752502441, -0.27156633138656616, 0.01748797297477722, 1.3395498991012573, 1.05559504032135, 0.5666554570198059],
    ...
  ]
>
```

```elixir
exla_gpu_mult_fn = fn -> Nx.dot(x_valid_cuda, weights_cuda) end
```

<!-- livebook:{"output":true} -->

```
#Function<43.3316493/0 in :erl_eval.expr/6>
```

```elixir
repeat_times = 5
# Warm up one iteration
{elapsed_time_micro, _} = :timer.tc(repeat, [exla_gpu_mult_fn, repeat_times])
{elapsed_time_micro, _} = :timer.tc(repeat, [exla_gpu_mult_fn, repeat_times])
avg_elapsed_time_ms = elapsed_time_micro / 1000 / repeat_times

{backend, [device: device]} = Nx.default_backend()

"#{backend} #{device} avg time in #{avg_elapsed_time_ms} milliseconds total_time #{elapsed_time_micro / 1000} milliseconds"
```

<!-- livebook:{"output":true} -->

```
"Elixir.EXLA.Backend cuda avg time in 0.1402 milliseconds total_time 0.701 milliseconds"
```

```elixir
x_valid_cpu = Nx.backend_transfer(x_valid_cuda, Nx.BinaryBackend)
weights_cpu = Nx.backend_transfer(weights_cuda, Nx.BinaryBackend)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[784][10]
  [
    [-0.5412212014198303, -1.5031448602676392, -0.9375351071357727, 0.8842236995697021, 1.2010047435760498, -0.2788948118686676, 0.09495414793491364, -1.530068039894104, -1.0373481512069702, 1.0582575798034668],
    [1.0582325458526611, 0.623931884765625, -2.1486785411834717, 0.33179396390914917, 0.39910393953323364, 0.36134496331214905, -1.6878384351730347, -1.3250863552093506, 0.9663388729095459, -1.22098708152771],
    [-1.0654748678207397, 0.3679066598415375, -1.8295735120773315, -0.333093523979187, 1.6659176349639893, 0.5998020768165588, -0.2500625252723694, -1.2196239233016968, 0.1907813549041748, -0.14031249284744263],
    [-0.8476660847663879, 0.3183368742465973, -0.01679874397814274, -0.33443644642829895, 0.2650495767593384, 0.4326851963996887, -0.673367440700531, -0.07346426695585251, -0.2923451066017151, 1.0891989469528198],
    [0.7539434432983398, -1.7904051542282104, -0.24815252423286438, -0.42723143100738525, -0.8594269752502441, -0.27156633138656616, 0.01748797297477722, 1.3395498991012573, 1.05559504032135, 0.5666554570198059],
    ...
  ]
>
```
